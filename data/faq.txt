CSBrainAI - Frequently Asked Questions

=== General Questions ===

Q: What is CSBrainAI?
A: CSBrainAI is a privacy-focused Retrieval Augmented Generation (RAG) system that answers questions based on your knowledge base using AI. It combines vector search (Supabase + pgvector) with large language models (OpenAI GPT-4) to provide accurate, context-aware answers.

Q: Is my data private?
A: Yes! We take privacy seriously. CSBrainAI never logs raw query text. We only store HMAC hashes and query lengths. All PII is scrubbed before sending to Sentry. See our PRIVACY.md for details.

Q: How much does it cost to run?
A: Costs depend on usage:
- OpenAI API: ~$0.0001 per query (embeddings + gpt-4o-mini)
- Supabase: Free tier supports ~500k vectors, paid plans start at $25/month
- Vercel/hosting: Free tier for small projects

Q: What file formats are supported?
A: Currently .md (Markdown) and .txt (plain text) files. Future versions may support PDF, HTML, and DOCX.

=== Technical Questions ===

Q: How does vector search work?
A: When you ingest documents, we:
1. Split them into chunks (1000 chars with 200 overlap)
2. Generate embeddings (1536-dim vectors) via OpenAI
3. Store in Supabase with pgvector
4. Query time: embed your question, find top-5 similar chunks, generate answer

Q: What's the embedding model?
A: We use OpenAI's text-embedding-3-small (1536 dimensions). It's cost-effective and performs well for most use cases.

Q: What's the LLM for generation?
A: gpt-4o-mini by default. It's 10x cheaper than GPT-4 with excellent quality for Q&A.

Q: Can I use my own models?
A: Not yet, but we're considering support for:
- Local embeddings (sentence-transformers)
- Local LLMs (Ollama, llama.cpp)
- Other providers (Anthropic, Cohere)

Q: How fast are queries?
A: Typically 1-3 seconds end-to-end:
- Vector search: ~100ms
- Embedding: ~200ms
- LLM generation: 500-2000ms

Q: What's the rate limit?
A: 10 requests per minute per IP. Configurable via Upstash Redis for distributed environments.

=== Data Management ===

Q: How do I add new documents?
A: Place .md or .txt files in the /data directory and run:
   npm run ingest

   The system will:
   - Chunk new documents
   - Generate embeddings
   - Upsert to Supabase (deduplicates via chunk_hash)

Q: How do I update existing documents?
A: Edit the file in /data and re-run ingest. The chunk_hash ensures we don't create duplicates.

Q: How do I delete documents?
A: Connect to Supabase and delete rows from rag_docs table:
   DELETE FROM rag_docs WHERE source_url = 'data/old-file.md';

Q: What's the maximum document size?
A: No hard limit, but very large files will be split into many chunks. We recommend <100KB per file for best results.

=== Deployment ===

Q: Where can I deploy?
A: Any Node.js hosting:
- Vercel (recommended for Next.js)
- AWS Lambda / ECS
- Google Cloud Run
- DigitalOcean App Platform
- Self-hosted (Docker)

Q: Do I need Upstash Redis?
A: No, it's optional. Without it, we use in-memory rate limiting (works fine for single-instance deployments).

Q: How do I set up Sentry?
A: 1. Create account at sentry.io
   2. Create new project (Next.js)
   3. Copy DSN to .env:
      SENTRY_DSN=https://xxx@xxx.ingest.sentry.io/xxx
      NEXT_PUBLIC_SENTRY_DSN=https://xxx@xxx.ingest.sentry.io/xxx

Q: How do I migrate the database?
A: Run the SQL migration against your Supabase database:
   psql $SUPABASE_URL < supabase/migrations/001_init_rag.sql

   Or use Supabase dashboard: SQL Editor → paste migration → run.

=== Troubleshooting ===

Q: I'm getting "OPENAI_API_KEY not configured" error
A: Make sure .env.local exists with:
   OPENAI_API_KEY=sk-...

   Restart your dev server after adding it.

Q: Ingest script fails with "SUPABASE_SERVICE_ROLE not configured"
A: You need the service role key (not anon key) for writes. Get it from:
   Supabase Dashboard → Settings → API → service_role key

Q: Vector search returns no results
A: Check:
   1. Did ingest complete successfully?
   2. Is the ivfflat index created? (check pgvector docs)
   3. Is min_similarity too high? Try lowering to 0.3

Q: API returns 429 errors constantly
A: You're hitting rate limits. Either:
   - Add Upstash Redis for distributed rate limiting
   - Increase RATE_LIMIT_CONFIG.maxRequests in rate-limit.ts
   - Wait 60 seconds between bursts

Q: Sentry not capturing errors
A: Check:
   1. Is SENTRY_DSN set correctly?
   2. Is instrumentation.ts loaded? (should be automatic in Next.js 13+)
   3. Check Sentry dashboard for blocked events

=== Performance Optimization ===

Q: How can I make queries faster?
A: 1. Use Vercel Edge Functions for lower latency
   2. Cache embeddings (same query → same embedding)
   3. Reduce top-k from 5 to 3 (fewer docs to process)
   4. Use streaming responses (future feature)

Q: How can I reduce costs?
A: 1. Use smaller context windows (fewer top-k results)
   2. Cache responses for common queries
   3. Use gpt-4o-mini instead of GPT-4
   4. Batch ingest operations

Q: Can I scale to millions of documents?
A: Yes, but you'll need to:
   1. Tune ivfflat index (increase lists parameter)
   2. Use Supabase Pro tier
   3. Consider sharding by category
   4. Implement hierarchical retrieval

=== Contributing ===

Q: Can I contribute to CSBrainAI?
A: Yes! We welcome contributions. Check GitHub for open issues and submit PRs.

Q: What's the roadmap?
A: Planned features:
   - Hybrid search (BM25 + vector)
   - Streaming responses
   - Multi-tenant support
   - PDF/DOCX ingestion
   - A/B testing framework
   - Feedback collection (thumbs up/down)

Q: How do I report a bug?
A: Submit an issue on GitHub with:
   - Steps to reproduce
   - Expected vs actual behavior
   - Error messages (with PII redacted)
   - Environment (Node version, OS, etc.)
